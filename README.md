# Connect 4 RL Solving ğŸ®ğŸ§ 

The goal of the repo is to try different RL algorithms to play connect4 (model free). This is an extension to my Roboticc arm pplaying connect4 project, that explores more deeply the different algorithms that can solve the task. 

You can train your own models with customed parameters for each RL algorithm directly in the UI. The purpose of this is to offer an easy way for people to explore RL algorithms, and add their own, following the same structure as DQN example. 

This project is in developpement. For the moment, the algorithms avaible are:
- Double DQN


## ğŸŒŸ Core components

- **/scripts/Env.py**: The Connect4 environnement
- **/scripts/Train.py**: Training scripts for algorithms
- **/scripts/Connect4.py**: The basic rules for the connect4 (useful for the env)
- **/scripts/rl_algorithms**: The folder containing the avaible algorithms (in which you can create yours).
- **/graphics**: The folder containing the graphics part
- **Experience Replay**: Efficient learning through stored game experiences
- **Epsilon-Greedy Strategy**: Balanced exploration vs exploitation
- **Model Persistence**: Save and load trained models
- **GUI Interface**: Interactive game interface using Kivy

## ğŸ—ï¸ Architecture

### Core Components

| File | Description |
|------|-------------|
| `DQN2.py` | Enhanced Deep Q-Network implementation with TensorFlow/Keras |
| `Train2.py` | Training pipeline for multi-agent self-play |
| `Connect4.py` | Core Connect 4 game logic and board management |
| `env.py` | Game environment following OpenAI Gym-style interface |
| `connect4InterfaceNoRobot.py` | GUI interface for human vs AI gameplay |

### Neural Network Architecture

```
Input Layer (42 neurons) 
    â†“
One-Hot Encoding (42 â†’ 126 features)
    â†“ 
Dense Layer (n_neurons Ã— 3)
    â†“
Batch Normalization â†’ ReLU â†’ Dropout (0.2)
    â†“
[Hidden Layers] Ã— (n_layers - 2)
    â†“
Dense Layer (n_neurons)
    â†“
Batch Normalization â†’ ReLU
    â†“
Output Layer (42 neurons, softmax)
```

## ğŸš€ Getting Started

### Prerequisites

```bash
# Required packages
tensorflow>=2.16.1
numpy
kivy  # For GUI interface
collections  # For replay buffer
```

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd connect_4_dqn
   ```

2. **Set up Python environment**
   ```bash
   # Create conda environment (recommended)
   conda create -n connect4_dqn python=3.10
   conda activate connect4_dqn
   
   # Install dependencies
   pip install tensorflow numpy kivy
   ```

3. **Create models directory**
   ```bash
   mkdir -p models
   ```

### Quick Start

#### 1. Test the DQN Implementation
```bash
python DQN2.py
```

#### 2. Train AI Agents
```bash
python Train2.py
```

#### 3. Play Against AI
```bash
python connect4InterfaceNoRobot.py
```

## ğŸ¯ Usage

### Training Configuration

```python
# Example training setup
trainer = Train(
    model_name="my_model",
    learning_rate=0.5e-3,
    discount_factor=0.98,
    eps=0.5,  # Initial epsilon for exploration
    reset=False  # Set to True to start fresh training
)

# Train for n games
trainer.train_n_games(1000)
```

### DQN Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `n_layers` | 2 | Number of hidden layers |
| `n_neurons` | 32 | Neurons per hidden layer |
| `learning_rate` | 1e-2 | Adam optimizer learning rate |
| `gamma` | 1e-1 | Discount factor for future rewards |
| `eps` | 0.9 | Initial epsilon for Îµ-greedy strategy |
| `batch_size` | 32 | Experience replay batch size |

## ğŸ§  Deep Q-Learning Details

### ğŸ¯ Learning Process Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Game State    â”‚â”€â”€â”€â–¶â”‚   DQN Agent     â”‚â”€â”€â”€â–¶â”‚     Action      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  [ 0 0 0 0 0 0 0] â”‚    â”‚  Neural Network â”‚    â”‚   Column: 3     â”‚
â”‚  [ 0 0 0 0 0 0 0] â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  [ 0 0 0 0 0 0 0] â”‚    â”‚ Q-Values for    â”‚    â”‚                 â”‚
â”‚  [ 0 0 0 1 0 0 0] â”‚    â”‚ each column     â”‚    â”‚                 â”‚
â”‚  [ 0 0 2 1 0 0 0] â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  [ 0 1 2 1 2 0 0] â”‚    â”‚ [0.1,0.3,0.2,   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  0.8,0.1,0.2,0.1]â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Experience    â”‚
                       â”‚     Storage     â”‚
                       â”‚                 â”‚
                       â”‚ (state, action, â”‚
                       â”‚ reward, next_   â”‚
                       â”‚ state, done)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ® Self-Play Training Cycle

#### Step 1: Initial Game State
```
Connect 4 Board (Empty):
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â† Row 5
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â† Row 4
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â† Row 3
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â† Row 2
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â† Row 1
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â† Row 0
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
  0   1   2   3   4   5   6   â† Columns
```

#### Step 2: Player 1 (Red) Makes Move
```
Agent 1 Decision Process:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Current State:  â”‚
â”‚ [0,0,0,0,0,0,0, â”‚ â† Flattened board (42 elements)
â”‚  0,0,0,0,0,0,0, â”‚
â”‚  0,0,0,0,0,0,0, â”‚
â”‚  0,0,0,0,0,0,0, â”‚
â”‚  0,0,0,0,0,0,0, â”‚
â”‚  0,0,0,0,0,0,0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ (Neural Network Processing)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Q-Values:       â”‚
â”‚ Col 0: 0.12     â”‚ â† Low probability
â”‚ Col 1: 0.08     â”‚
â”‚ Col 2: 0.15     â”‚
â”‚ Col 3: 0.25     â”‚ â—„â”€â”€ HIGHEST! Choose this
â”‚ Col 4: 0.18     â”‚
â”‚ Col 5: 0.11     â”‚
â”‚ Col 6: 0.11     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ (Action: Drop in Column 3)
         
Result Board:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚ 1 â”‚   â”‚   â”‚   â”‚ â† Player 1 piece
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
  0   1   2   3   4   5   6
```

#### Step 3: Player 2 (Yellow) Responds
```
Agent 2 sees updated board and decides:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚ 2 â”‚   â”‚   â”‚   â”‚ â† Player 2 blocks
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
                â”‚
                â–¼ (Strategic blocking move)
```

#### Step 4: Learning from Outcomes
```
Game Progression Example:

Move 1:           Move 5:           Final State:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    1    â”‚      â”‚ 2   1   â”‚       â”‚ 2 1 1 2 â”‚ â† Player 1 WINS!
â”‚    2    â”‚ â”€â”€â”€â–¶ â”‚ 1 2 2 1 â”‚ â”€â”€â”€â–¶  â”‚ 1 2 2 1 â”‚   (4 in a row)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ 2 1 1 2 â”‚       â”‚ 2 1 1 2 â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Experience Storage:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stateâ‚ â†’ Actionâ‚ â†’ Rewardâ‚ â†’ Stateâ‚‚ â†’ Done             â”‚
â”‚ [0,0,0,1,0,0,0...] â†’ 3 â†’ +1.0 â†’ [final] â†’ True        â”‚ â† Win!
â”‚                                                         â”‚
â”‚ Stateâ‚ â†’ Actionâ‚ â†’ Rewardâ‚ â†’ Stateâ‚‚ â†’ Done             â”‚
â”‚ [0,0,0,2,0,0,0...] â†’ 3 â†’ -1.0 â†’ [final] â†’ True        â”‚ â† Loss!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Experience Replay & Learning

```
Training Batch (Random Sample from Memory):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Experience 1: [state] â†’ action: 3 â†’ reward: +1.0 â†’ [next]  â”‚
â”‚ Experience 2: [state] â†’ action: 1 â†’ reward: -0.1 â†’ [next]  â”‚
â”‚ Experience 3: [state] â†’ action: 4 â†’ reward: +0.5 â†’ [next]  â”‚
â”‚ Experience 4: [state] â†’ action: 2 â†’ reward: -1.0 â†’ [next]  â”‚
â”‚                           ...                               â”‚
â”‚ Experience 32: [state] â†’ action: 6 â†’ reward: +0.0 â†’ [next] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ (Batch Learning)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Q-Learning Update                        â”‚
â”‚                                                             â”‚
â”‚ Target Q-Value = Reward + Î³ Ã— max(Q_next_state)           â”‚
â”‚                                                             â”‚
â”‚ Current Q-Value = Neural_Network(current_state)[action]     â”‚
â”‚                                                             â”‚
â”‚ Loss = MSE(Target Q-Value, Current Q-Value)                â”‚
â”‚                                                             â”‚
â”‚ Backpropagation: Update network weights to minimize loss   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ Epsilon-Greedy Strategy Evolution

```
Training Progress:

Episode 1 (Îµ = 0.9):           Episode 500 (Îµ = 0.3):         Episode 1000 (Îµ = 0.1):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 90% Random      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ 30% Random      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ 10% Random      â”‚
â”‚ 10% Best Action â”‚           â”‚ 70% Best Action â”‚            â”‚ 90% Best Action â”‚
â”‚                 â”‚           â”‚                 â”‚            â”‚                 â”‚
â”‚ Exploration     â”‚           â”‚ Balanced        â”‚            â”‚ Exploitation    â”‚
â”‚ Heavy Learning  â”‚           â”‚ Learning        â”‚            â”‚ Optimal Play    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Random Move Example:          Neural Network Move:           Expert Move:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â” â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚ 2 â”‚   â”‚   â”‚   â”‚ â”‚   â”‚   â”‚   â”‚ 2 â”‚   â”‚   â”‚   â”‚  â”‚   â”‚   â”‚   â”‚ 2 â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚ 1 â”‚   â”‚   â”‚   â”‚ â”‚   â”‚   â”‚   â”‚ 1 â”‚   â”‚   â”‚   â”‚  â”‚   â”‚   â”‚   â”‚ 1 â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚ 1 â”‚   â”‚ 2 â”‚   â”‚   â”‚   â”‚ â”‚   â”‚   â”‚ 1 â”‚ 2 â”‚   â”‚   â”‚   â”‚  â”‚   â”‚   â”‚   â”‚ 2 â”‚ 1 â”‚   â”‚   â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
  Random move: Col 1 (bad)     Smart move: Col 2 (good)      Expert: Col 4 (blocks win!)
```

### ğŸ§® Neural Network Processing Flow

```
Input Processing:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Connect 4 Board â”‚â”€â”€â”€â–¶â”‚   One-Hot       â”‚â”€â”€â”€â–¶â”‚   Flattened     â”‚
â”‚                 â”‚    â”‚   Encoding      â”‚    â”‚   Vector        â”‚
â”‚ [0,1,2,0,1,2,0, â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  0,0,0,0,0,0,0, â”‚    â”‚ 0â†’[1,0,0]      â”‚    â”‚ [1,0,0,0,1,0,   â”‚
â”‚  0,0,0,0,0,0,0, â”‚    â”‚ 1â†’[0,1,0]      â”‚    â”‚  0,0,1,1,0,0,   â”‚
â”‚  0,0,0,0,0,0,0, â”‚    â”‚ 2â†’[0,0,1]      â”‚    â”‚  0,1,0,0,0,0,   â”‚
â”‚  0,0,0,0,0,0,0, â”‚    â”‚                 â”‚    â”‚  ...           â”‚
â”‚  0,0,0,0,0,0,0] â”‚    â”‚                 â”‚    â”‚  126 features] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       42 values              3D encoding           126 features

Hidden Layer Processing:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense Layer 1   â”‚â”€â”€â”€â–¶â”‚ Batch Norm +    â”‚â”€â”€â”€â–¶â”‚ Dense Layer 2   â”‚
â”‚                 â”‚    â”‚ ReLU + Dropout  â”‚    â”‚                 â”‚
â”‚ 126 â†’ 96 nodes  â”‚    â”‚                 â”‚    â”‚ 96 â†’ 32 nodes   â”‚
â”‚                 â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚                 â”‚
â”‚ Wâ‚ Ã— input + bâ‚ â”‚    â”‚ â”‚ 20% dropout â”‚ â”‚    â”‚ Wâ‚‚ Ã— hâ‚ + bâ‚‚   â”‚
â”‚                 â”‚    â”‚ â”‚ (training)  â”‚ â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output Generation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Final Dense     â”‚â”€â”€â”€â–¶â”‚ Q-Values for    â”‚
â”‚                 â”‚    â”‚ Each Action     â”‚
â”‚ 32 â†’ 7 nodes    â”‚    â”‚                 â”‚
â”‚                 â”‚    â”‚ [Qâ‚€, Qâ‚, Qâ‚‚,   â”‚
â”‚ Softmax/Linear  â”‚    â”‚  Qâ‚ƒ, Qâ‚„, Qâ‚…,   â”‚
â”‚ Activation      â”‚    â”‚  Qâ‚†]            â”‚
â”‚                 â”‚    â”‚                 â”‚
â”‚                 â”‚    â”‚ Action = argmax â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State Representation
- **Board State**: 6Ã—7 grid flattened to 42-element vector
- **Encoding**: 0 (empty), 1 (player 1), 2 (player 2)
- **One-Hot**: Each position expanded to 3-dimensional one-hot vector

### Action Space
- **Actions**: 7 possible column choices (0-6)
- **Invalid Moves**: Handled by environment with negative rewards

### Reward System
- **Win**: +1 reward
- **Loss**: -1 reward  
- **Draw**: 0 reward
- **Invalid Move**: Negative penalty
- **Ongoing**: Small step penalty to encourage faster wins

### Training Process
1. **Self-Play**: Two DQN agents play against each other
2. **Experience Collection**: Store (state, action, reward, next_state, done) tuples
3. **Replay Buffer**: Maintain buffer of recent experiences
4. **Batch Learning**: Sample random batches for training
5. **Target Network**: Separate target network for stable learning

## ğŸ“Š Model Performance

### Training Metrics
- **Episode Rewards**: Track cumulative rewards per game
- **Win Rate**: Percentage of games won vs random/previous versions
- **Loss Convergence**: Monitor training loss reduction
- **Epsilon Decay**: Exploration rate reduction over time

### Evaluation
```python
# Evaluate trained model
dqn = DQN("trained_model")
state = np.array([0] * 42)  # Empty board
action_probs = dqn.model.predict(state[np.newaxis])[0]
best_action = np.argmax(action_probs)
```

## ğŸ® Game Interface

The GUI interface (`connect4InterfaceNoRobot.py`) provides:
- **Interactive Board**: Click to drop pieces
- **AI Opponent**: Play against trained DQN
- **Visual Feedback**: Real-time game state updates
- **Score Tracking**: Win/loss statistics

## ğŸ”§ Troubleshooting

### Common Issues

**1. TensorFlow Import Errors**
```bash
# Use tf.keras instead of separate keras import
import tensorflow as tf
# All keras functionality via tf.keras.*
```

**2. Model Loading Issues**
```bash
# Ensure custom layers are registered
custom_objects = {'OneHotLayer': OneHotLayer}
model = tf.keras.models.load_model(path, custom_objects=custom_objects)
```

**3. GPU Setup (Optional)**
```bash
# Check GPU availability
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

### Performance Tips
- **CPU Training**: Works well for Connect 4 complexity
- **Batch Size**: Increase for faster training (if memory allows)
- **Learning Rate**: Lower for more stable convergence
- **Replay Buffer**: Larger buffer for more diverse experiences

## ğŸ“ˆ Future Improvements

- [ ] **Advanced Architectures**: Convolutional layers for spatial awareness
- [ ] **Tournament Play**: Multi-agent tournaments for robust evaluation  
- [ ] **Opening Book**: Pre-computed optimal opening moves
- [ ] **Alpha-Beta Integration**: Hybrid AI with traditional game tree search
- [ ] **Web Interface**: Browser-based gameplay
- [ ] **Model Compression**: Smaller models for deployment

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **DeepMind**: Original DQN paper and methodology
- **OpenAI Gym**: Environment interface inspiration
- **TensorFlow/Keras**: Deep learning framework
- **Connect 4 Community**: Game rules and strategy insights

## ğŸ“ Contact

- **Author**: [Your Name]
- **Email**: [your.email@example.com]
- **Project Link**: [https://github.com/yourusername/connect_4_dqn](https://github.com/yourusername/connect_4_dqn)

---

*Built with â¤ï¸ and lots of â˜• for the love of AI and classic games*
